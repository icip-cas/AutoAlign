import streamlit as st
import os
import time
st.set_page_config(layout="centered")
nav_cols = st.columns(4)
for key, default in {
    "p1_fin": False,
    "p2_fin": False,
    "p3_fin": False,
    "p4_fin": False,
    "selected_button": "data_gen"
}.items():
    if key not in st.session_state:
        st.session_state[key] = default

with nav_cols[0]:
    if st.button("ğŸ“¥ Generate Query", use_container_width=True):
        st.session_state.selected_button = "data_gen"
with nav_cols[1]:
    if st.button("ğŸ” Sample Answer", use_container_width=True):
        st.session_state.selected_button = "data_filter"
with nav_cols[2]:
    if st.button("ğŸ“ Train", use_container_width=True) and st.session_state.p2_fin:
        st.session_state.selected_button = "train"
with nav_cols[3]:
    if st.button("ğŸ“Š Eval", use_container_width=True) and st.session_state.p2_fin and st.session_state.p3_fin:
        st.session_state.selected_button = "eval"

st.title("Model Eval")

# åˆå§‹åŒ– session_state
if "process" not in st.session_state:
    st.session_state["process"] = "objective_all"
if "model_dir" not in st.session_state:
    st.session_state["model_dir"] = ""
if "output_dir" not in st.session_state:
    st.session_state["output_dir"] = ""
if "model_name" not in st.session_state:
    st.session_state["model_name"] = ""  # åˆå§‹åŒ–ä¸ºç©ºå­—ç¬¦ä¸²
if "per_model_gpu" not in st.session_state:
    st.session_state["per_model_gpu"] = 1
if "batch_size" not in st.session_state:
    st.session_state["batch_size"] = 8

# é…ç½®è¡¨å•
with st.form("config_form"):
    st.subheader("BenchMark")
    col1, col2 = st.columns([3, 2])
    with col1:
        process = st.selectbox(
            "BenchMark",
            [
                "objective_core",
                "objective_all",
                "subjective",
            ],
            index=[
                "objective_core",
                "objective_all",
                "subjective",
            ].index(st.session_state["process"]),  # æ¢å¤ process çš„é€‰æ‹©çŠ¶æ€
            label_visibility="collapsed"
        )

    st.subheader("Model Selection")
    model_dir = st.text_input(
        "Model Dir", 
        placeholder="Please provide the path for the model.", 
        value=st.session_state["model_dir"],  # æ¢å¤ model_dir çš„è¾“å…¥å†…å®¹
        label_visibility="collapsed"
    )

    # è¯„æµ‹çš„æ¨¡å‹æ ‡è¯†åç§°
    st.subheader("Model Name")
    model_name = st.text_input(
        "Model Name", 
        placeholder="Enter the identifying name of the model to evaluate.", 
        value=st.session_state["model_name"],  # ä½¿ç”¨ session_state ä¸­çš„å€¼
        label_visibility="collapsed"
    )

    # GPU å’Œ Batch Size é…ç½®
    st.subheader("GPU and Batch Size Configuration")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**GPU per Model**")
        per_model_gpu = st.number_input(
            "GPU per Model", 
            min_value=1, 
            value=st.session_state["per_model_gpu"],
            step=1,
            label_visibility="collapsed"
        )
    with col2:
        st.markdown("**Batch Size**")
        batch_size = st.number_input(
            "Batch Size", 
            min_value=1, 
            value=st.session_state["batch_size"],  # æ¢å¤ batch_size çš„è¾“å…¥å†…å®¹
            step=1,
            label_visibility="collapsed"
        )

    # æäº¤æŒ‰é’®
    col1, col2, col3 = st.columns([4, 2, 4])
    with col2:
        submitted = st.form_submit_button("ğŸš€ Start")

    # è¡¨å•æäº¤åçš„é€»è¾‘
    if submitted:
        # ä¿å­˜ç”¨æˆ·è¾“å…¥åˆ° session_state
        st.session_state["process"] = process
        st.session_state["model_dir"] = model_dir
        st.session_state["model_name"] = model_name
        st.session_state["per_model_gpu"] = per_model_gpu
        st.session_state["batch_size"] = batch_size

        all_fields_filled = True

        # æ£€æŸ¥ Model Dir æ˜¯å¦å­˜åœ¨
        if not model_dir:
            st.error("Please provide the model directory.")
            all_fields_filled = False
        elif not os.path.exists(model_dir):
            st.error(f"Model directory '{model_dir}' does not exist.")
            all_fields_filled = False

        # æ£€æŸ¥ Model Name æ˜¯å¦ä¸ºç©º
        if not model_name:
            st.error("Please provide the model name.")
            all_fields_filled = False

        # å¦‚æœæ‰€æœ‰å­—æ®µåˆæ³•ä¸”è·¯å¾„æ£€æŸ¥é€šè¿‡
        if all_fields_filled:
            # æ ¹æ®è¯„æµ‹ç±»å‹è®¾ç½® mt_path å’Œ alpaca_path
            mt_path = "data/eval/mt-bench"
            alpaca_path = "data/eval/alpaca_eval" if process == "subjective" else None

            # ç”Ÿæˆé…ç½®æ–‡ä»¶å†…å®¹
            config_content = f"""# è¯„æµ‹çš„æ¨¡å‹æ ‡è¯†åç§°
# Identifying name of the model to evaluate
model_name: {model_name}

# è¯„æµ‹æ—¶ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ¨¡æ¿ï¼Œå¯è§src/autoalign/conversation.pyä¸­çš„TEMPLATES
template_name: chatml-keep-system
# è¯„æµ‹çš„æ¨¡å‹è·¯å¾„
# The path of the model to evaluate
model_path: {model_dir}
# è¯„æµ‹çš„ç±»å‹
# The type of evaluation
# å¯é€‰é¡¹ï¼š
# objective_core: è¯„æµ‹æ¨¡å‹çš„æ ¸å¿ƒå®¢è§‚æŒ‡æ ‡ï¼Œæ˜¯objective_allå¯¹åº”æŒ‡æ ‡çš„çœŸå­é›†ã€‚(evaluating the core objective metrics, a subset of the metrics in objective_all, of the model)
# objective_all: è¯„æµ‹æ¨¡å‹çš„æ‰€æœ‰å®¢è§‚æŒ‡æ ‡ã€‚(evaluating all the objective metrics of the model)
# subjective: è¯„æµ‹æ¨¡å‹çš„ä¸»è§‚æŒ‡æ ‡ã€‚(evaluating the subjective metrics of the model)
eval_type: {process}
# å•ä¸ªæ¨¡å‹ worker æ‰€å ç”¨çš„GPUæ•°é‡
# The number of GPUs occupied by a single model worker
per_model_gpu: {per_model_gpu}

# å•ä¸ª worker çš„ batch_size
# The batch size of a single worker
batch_size: {batch_size}

# æ¨ç† backend
# The inference backend
backend: vllm

# ==============Opencompass è®¾ç½®================
# opencompassæ–‡ä»¶å¤¹çš„è·¯å¾„
# The path of opencompass
opencompass_path: opencompass

# ==============MTbench è®¾ç½®================
# mtbenchæ–‡ä»¶å¤¹çš„è·¯å¾„
mt_path: {mt_path}

# Recommend using: chatgpt_fn or weighted_alpaca_eval_gpt4_turbo
# use weighted_alpaca_eval_gpt4_turbo if you want the high agreement with humans.
# use chatgpt_fn if you are on a tight budget.
judge_model: chatgpt_fn
"""

            if alpaca_path:
                config_content += f"""
# ==============AlpacaEval è®¾ç½®================
# see https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md
# æŒ‡å®šAlpacaEvalæ–‡ä»¶çš„è·¯å¾„(setting the alpaca eval file path if you have already downloaded it)
alpaca_path: {alpaca_path}
"""

            current_dir = os.path.dirname(__file__)

            relative_path = os.path.join("..", "configs") 
            target_dir = os.path.normpath(os.path.join(current_dir, relative_path))

            target_file = os.path.join(target_dir, "eval.yaml")
            try:
                with open(target_file, "w") as f:
                    f.write(config_content)
                st.success(f"The configuration has been successfully saved.")
                time.sleep(1.5)
                st.switch_page("page5.py")
            except Exception as e:
                st.error(f"Failed to save configuration")

if st.session_state.selected_button == "data_gen":
    st.switch_page("page1.py")
elif st.session_state.selected_button == "data_filter":
    st.switch_page("page2.py")
elif st.session_state.selected_button == "train":
    st.switch_page("page3.py")
elif st.session_state.selected_button == "eval":
    pass