import streamlit as st
import os

st.title("ğŸ“Š Model Evaluation")

# åˆå§‹åŒ– session_state
if "process" not in st.session_state:
    st.session_state["process"] = "objective_all"
if "model_dir" not in st.session_state:
    st.session_state["model_dir"] = ""
if "output_dir" not in st.session_state:
    st.session_state["output_dir"] = ""
if "model_name" not in st.session_state:
    st.session_state["model_name"] = ""  # åˆå§‹åŒ–ä¸ºç©ºå­—ç¬¦ä¸²
if "per_model_gpu" not in st.session_state:
    st.session_state["per_model_gpu"] = 1
if "batch_size" not in st.session_state:
    st.session_state["batch_size"] = 8


process_descriptions = {
    "objective_core": "GSM-8K(EN)ï¼ŒMATH(EN)ï¼ŒHumanEval(EN)ï¼ŒHumanEval-CN(CH)ï¼ŒBBH(EN)ï¼ŒIFEval(EN)ï¼ŒCONFIG_ALLï¼š",
    "objective_all": "GSM-8K(EN)ï¼ŒMATH(EN)ï¼ŒHumanEval(EN)HumanEval-CN(CH)ï¼ŒBBH(EN)ï¼ŒIFEval(EN)ï¼ŒCMMLU(CH)ï¼ŒC-Eval(CH)ï¼ŒMBPP(EN)ï¼ŒMBPP-CN(CH)ï¼ŒGPQA(EN)",
    "subjective": "MT-Bench and Alpaca-Eval",
}

# ç¾åŒ–æ ‡é¢˜å’Œåˆ†éš”çº¿
st.markdown("---")
st.subheader("ğŸ“Š BenchMark")

# åœ¨è¡¨å•å¤–ä½¿ç”¨ st.selectboxï¼Œä»¥ä¾¿ä½¿ç”¨ on_change å›è°ƒ
process = st.selectbox(
    "é€‰æ‹©è¯„æµ‹ç±»å‹",
    options=[
        "objective_core",
        "objective_all",
        "subjective",
    ],
    index=[
        "objective_core",
        "objective_all",
        "subjective",
    ].index(st.session_state["process"]),  # æ¢å¤ process çš„é€‰æ‹©çŠ¶æ€
    key="process_selectbox",
    on_change=lambda: st.session_state.update(
        {"process": process}
    ),  # æ›´æ–° session_state
)

# æ ¹æ® session_state ä¸­çš„ process åŠ¨æ€æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
st.markdown(
    f"""
    <style>
    .info-box {{
        padding: 15px;
        background-color: #f0f2f6;
        border-radius: 10px;
        border-left: 5px solid #4a90e2;
        margin-top: 10px;
        margin-bottom: 20px;
    }}
    .info-box p {{
        margin: 0;
        font-size: 14px;
        color: #333;
    }}
    </style>
    <div class="info-box">
        <p>ğŸ“Œ <strong>Benchmark:</strong> {process_descriptions.get(st.session_state["process"], "æš‚æ— æè¿°")}</p>
    </div>
    """,
    unsafe_allow_html=True,
)

# é…ç½®è¡¨å•
with st.form("config_form"):
    st.markdown("---")
    st.subheader("ğŸ“‚ Model Selection")

    # æ¨¡å‹è·¯å¾„è¾“å…¥
    model_dir = st.text_input(
        "æ¨¡å‹è·¯å¾„",
        placeholder="è¯·è¾“å…¥æ¨¡å‹è·¯å¾„",
        value=st.session_state["model_dir"],  # æ¢å¤ model_dir çš„è¾“å…¥å†…å®¹
        label_visibility="collapsed",
    )

    # è¯„æµ‹çš„æ¨¡å‹æ ‡è¯†åç§°
    st.subheader("ğŸ·ï¸ Model Name")
    model_name = st.text_input(
        "æ¨¡å‹åç§°",
        placeholder="è¯·è¾“å…¥æ¨¡å‹æ ‡è¯†åç§°",
        value=st.session_state["model_name"],  # ä½¿ç”¨ session_state ä¸­çš„å€¼
        label_visibility="collapsed",
    )

    # GPU å’Œ Batch Size é…ç½®
    st.markdown("---")
    st.subheader("âš™ï¸ GPU and Batch Size Configuration")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**GPU per Model**")
        per_model_gpu = st.number_input(
            "æ¯ä¸ªæ¨¡å‹çš„ GPU æ•°é‡",
            min_value=1,
            value=st.session_state["per_model_gpu"],
            step=1,
            label_visibility="collapsed",
        )
    with col2:
        st.markdown("**Batch Size**")
        batch_size = st.number_input(
            "æ‰¹é‡å¤§å°",
            min_value=1,
            value=st.session_state["batch_size"],  # æ¢å¤ batch_size çš„è¾“å…¥å†…å®¹
            step=1,
            label_visibility="collapsed",
        )

    # æäº¤æŒ‰é’®
    st.markdown("---")
    submitted = st.form_submit_button("ğŸš€ Start Evaluation")

    # è¡¨å•æäº¤åçš„é€»è¾‘
    if submitted:
        # ä¿å­˜ç”¨æˆ·è¾“å…¥åˆ° session_state
        st.session_state["process"] = process
        st.session_state["model_dir"] = model_dir
        st.session_state["model_name"] = model_name
        st.session_state["per_model_gpu"] = per_model_gpu
        st.session_state["batch_size"] = batch_size

        all_fields_filled = True

        # æ£€æŸ¥ Model Dir æ˜¯å¦å­˜åœ¨
        if not model_dir:
            st.error("âŒ è¯·æä¾›æ¨¡å‹è·¯å¾„ã€‚")
            all_fields_filled = False
        elif not os.path.exists(model_dir):
            st.error(f"âŒ æ¨¡å‹è·¯å¾„ '{model_dir}' ä¸å­˜åœ¨ã€‚")
            all_fields_filled = False

        # æ£€æŸ¥ Model Name æ˜¯å¦ä¸ºç©º
        if not model_name:
            st.error("âŒ è¯·æä¾›æ¨¡å‹åç§°ã€‚")
            all_fields_filled = False

        # å¦‚æœæ‰€æœ‰å­—æ®µåˆæ³•ä¸”è·¯å¾„æ£€æŸ¥é€šè¿‡
        if all_fields_filled:
            # æ ¹æ®è¯„æµ‹ç±»å‹è®¾ç½® mt_path å’Œ alpaca_path
            mt_path = "data/eval/mt-bench"
            alpaca_path = "data/eval/alpaca_eval" if process == "subjective" else None

            # ç”Ÿæˆé…ç½®æ–‡ä»¶å†…å®¹
            config_content = f"""# è¯„æµ‹çš„æ¨¡å‹æ ‡è¯†åç§°
# Identifying name of the model to evaluate
model_name: {model_name}

# è¯„æµ‹æ—¶ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ¨¡æ¿ï¼Œå¯è§src/autoalign/conversation.pyä¸­çš„TEMPLATES
template_name: chatml-keep-system
# è¯„æµ‹çš„æ¨¡å‹è·¯å¾„
# The path of the model to evaluate
model_path: {model_dir}
# è¯„æµ‹çš„ç±»å‹
# The type of evaluation
# å¯é€‰é¡¹ï¼š
# objective_core: è¯„æµ‹æ¨¡å‹çš„æ ¸å¿ƒå®¢è§‚æŒ‡æ ‡ï¼Œæ˜¯objective_allå¯¹åº”æŒ‡æ ‡çš„çœŸå­é›†ã€‚(evaluating the core objective metrics, a subset of the metrics in objective_all, of the model)
# objective_all: è¯„æµ‹æ¨¡å‹çš„æ‰€æœ‰å®¢è§‚æŒ‡æ ‡ã€‚(evaluating all the objective metrics of the model)
# subjective: è¯„æµ‹æ¨¡å‹çš„ä¸»è§‚æŒ‡æ ‡ã€‚(evaluating the subjective metrics of the model)
eval_type: {process}
# å•ä¸ªæ¨¡å‹ worker æ‰€å ç”¨çš„GPUæ•°é‡
# The number of GPUs occupied by a single model worker
per_model_gpu: {per_model_gpu}

# å•ä¸ª worker çš„ batch_size
# The batch size of a single worker
batch_size: {batch_size}

# æ¨ç† backend
# The inference backend
backend: vllm

# ==============Opencompass è®¾ç½®================
# opencompassæ–‡ä»¶å¤¹çš„è·¯å¾„
# The path of opencompass
opencompass_path: opencompass

# ==============MTbench è®¾ç½®================
# mtbenchæ–‡ä»¶å¤¹çš„è·¯å¾„
mt_path: {mt_path}

# Recommend using: chatgpt_fn or weighted_alpaca_eval_gpt4_turbo
# use weighted_alpaca_eval_gpt4_turbo if you want the high agreement with humans.
# use chatgpt_fn if you are on a tight budget.
judge_model: chatgpt_fn
"""

            if alpaca_path:
                config_content += f"""
# ==============AlpacaEval è®¾ç½®================
# see https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md
# æŒ‡å®šAlpacaEvalæ–‡ä»¶çš„è·¯å¾„(setting the alpaca eval file path if you have already downloaded it)
alpaca_path: {alpaca_path}
"""

            current_dir = os.path.dirname(__file__)

            relative_path = os.path.join("..", "configs")
            target_dir = os.path.normpath(os.path.join(current_dir, relative_path))

            target_file = os.path.join(target_dir, "eval.yaml")
            try:
                with open(target_file, "w") as f:
                    f.write(config_content)
                st.success("âœ… é…ç½®æ–‡ä»¶å·²æˆåŠŸä¿å­˜ï¼")
            except Exception as e:
                st.error(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

            # è·³è½¬åˆ° page5.py
            st.switch_page("page5.py")
