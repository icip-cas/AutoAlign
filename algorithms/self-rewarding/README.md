# Self-rewarding

This method use language model itself to provide its own rewards during training via LLM-as-a-Judge prompting.

Specifically, the model to be optimized do the following steps iteratively:
1. Sample some instruction from an instruct model
2. Generate candidate responses to the instructions above
3. Assign rewards to its own generations via LLM-as-a-Judge prompting
4. Generate pairwise preference data according to the rewards
5. Using DPO to train on the preferences

This training both improves the instruction following capability of the model, as well as its reward-modeling ability across the iterations, which means the model is better able to assign rewards in future iterations for improving instruction following â€“ a kind of virtuous circle.

While this improvement likely saturates in realistic scenarios, it still allows for the possibility of continual improvement beyond the human preferences that are typically used to build reward models and instruction following models today.

## IFT Seed Data Preparation

IFT seed data can be download and sampled by running the following scripts:

```bash
# Assume you are at the folder of self-rewarding
bash scripts/ift_data_prepare.sh
```

## IFT

IFT script is in the folder of `scripts/ift.sh`. You need to set the following parameters: DATA_PATH, CONV_TEMPLATE, OUTPUT_DIR, MODEL_PATH. The following is an example script to ift.

```bash
# Assume you are at the folder of self-rewarding
export DATA_PATH=data/seed.json
export CONV_TEMPLATE=llama-3-instruct
export OUTPUT_DIR=saved_models/llama3-8b-ift
export MODEL_PATH=meta-llama/Meta-Llama-3-8B
bash scripts/ift.sh
```

## EFT Data Preparation

We need to use the model just finetuned at the last step (SFT baseline). In this step, we use the SFT baseline to generate output evaluations for each input, and accept the input-output pairs if the ranking of their scores agrees with the human rankings in the dataset.

```bash
# Assume you are at the folder of self-rewarding
bash scripts/eft_data_prepare.sh
```

## EFT

The parameters we should set is the same as ift script. Here is an example of EFT.

```bash
# Assume you are at the folder of self-rewarding
export DATA_PATH=data/llama3-8b/train_eft_data.json
export CONV_TEMPLATE=llama-3-instruct
export OUTPUT_DIR=saved_models/llama3-8b-eft
export MODEL_PATH=saved_models/llama3-8b-ift
bash scripts/eft.sh
```
## Inference iter1

Then we inference the EFT model to generate preference data for iterative dpo. This script will generate about 4000 preference data.

```bash
# Assume you are at the folder of self-rewarding
bash scripts/dpo_data_gen_iter1.sh
```
## DPO iter1

Then we use DPO and the generated preference data to optimize the EFT model to M2 model.

```bash
# Assume you are at the folder of self-rewarding
export DATA_PATH=exp/llama3-8b/eft/iter1/preference_data.json
export CONV_TEMPLATE=llama-3-instruct
export OUTPUT_DIR=saved_models/llama3-8b-eft-dpo-iter1
export MODEL_PATH=saved_models/llama3-8b-eft
bash scripts/dpo_iter1.sh
```

## Inference iter2

Then we inference the M2 model (the model optimized with DPO in the last step) to generate preference data in a new iteration. This script will generate about 7200 preference data.

```bash
# Assume you are at the folder of self-rewarding
bash scripts/dpo_data_gen_iter2.sh
```

## DPO iter2


Then we use DPO and the preference data generated by M2 model to optimize the M2 model to M3 model.

```bash
# Assume you are at the folder of self-rewarding
export DATA_PATH=exp/llama3-8b/eft/iter2/preference_data.json
export CONV_TEMPLATE=llama-3-instruct
export OUTPUT_DIR=saved_models/llama3-8b-eft-dpo-iter2
export MODEL_PATH=saved_models/llama3-8b-eft-dpo-iter1
bash scripts/dpo_iter2.sh
```

## Evaluation

1. Use opencompass to evaluate on objective datasets (have relatively fixed standard to judge the performance): IFEval, ARC-e, ARC-c, Helllaswag, SIQA, PIQA, GSM8K, MMLU, OpenBookQA, NQ
2. Use `autoalign-cli eval` to evaluate on subjective datasets: MT-Bench and Alpaca-Eval
3. Evaluation split of EFT data is used to judge the ability of LLM-as-a-judge.

We have implemented an integrated evaluation python script at `src/eval.py`.

### Objective Evaluation
For opencompass (objective evaluation), here is an example:
```bash
# Assume you are at the folder of self-rewarding
export MODEL_PATH=saved_models/llama3-8b-eft-dpo-iter2
export TEMPLATE_NAME=llama-3-instruct
export MODEL_NAME=llama3-8b-eft-dpo-iter2
export EVAL_TYPE=objective
bash scripts/run_eval.sh
```
Note: For this part of the dataset and each model, the results shown in the next section are the **better ones** between those with the dialogue embedded in the template and those without template. The above script is evaluation with template, while the following script has no template.

```bash
# Assume you are at the folder of self-rewarding
export MODEL_PATH=saved_models/llama3-8b-eft-dpo-iter2
export TEMPLATE_NAME=None
export MODEL_NAME=llama3-8b-eft-dpo-iter2
export EVAL_TYPE=objective
bash scripts/run_eval.sh
```
The result can be find in the newest subfolder in `outputs/{MODEL_NAME}` relative to the root folder of self-rewarding.

### Subjective Evaluation
For subjective evaluation, here is an example:
```bash
# Assume you are at the folder of self-rewarding
export MODEL_PATH=saved_models/llama3-8b-eft-dpo-iter2
export TEMPLATE_NAME=llama-3-instruct
export MODEL_NAME=llama3-8b-eft-dpo-iter2
export EVAL_TYPE=subjective
bash scripts/run_eval.sh
```
The answer file can be find in the newest subfolder in `data/alpaca/{MODEL_NAME}/{MODEL_NAME}.jsonl` or  `data/eval/mt-bench/model_answer/{MODEL_NAME}.jsonl` relative to the root folder of **auto-alignment**.

### LLM-as-a-Judge
For llm-as-a-judge evaluation here is an example:
```bash
# Assume you are at the folder of self-rewarding
export MODEL_PATH=saved_models/llama3-8b-eft-dpo-iter2
export TEMPLATE_NAME=llama-3-instruct
export MODEL_NAME=llama3-8b-eft-dpo-iter2
export EVAL_TYPE=llm-as-a-judge
bash scripts/run_eval.sh
```
The result will be showed at the stdout.

## Reference Performance

### Instruction following Evaluation

| Model | Dataset / Algorithm |MT-Bench | IFEval-Pr.(S) | IFEval-Ins.(S) | IFEval-Pr.(L) | IFEval-Ins.(L) | IFEval(Avg.)
| -- | -- | -- | -- | -- | -- | -- | --
| Llama-3-8b | Base(M0) | 1.86 | 23.48 | 35.61 | 26.43 | 39.45 | 31.24
| Llama-3-8b | IFT(SFT-baseline) | 5.46 | 36.6 | 47.12 | 41.59 | 52.76 | 44.52
| Llama-3-8b | EFT(M1) | 5.48 | 35.86 | 48.2 | 40.85	| 53.12 | 44.51
| Llama-3-8b | Self-Rewarding-iter1(M2) | 5.54	| 36.41 | 48.44 |	41.77 | 53.72 | 45.09
| Llama-3-8b | Self-Rewarding-iter1(M3) | 5.58	| 36.78 | 48.68 | 41.96 | 53.84 | 45.32
| Llama-3-8b | Instruct | 6.7	| 68.58 | 77.1 | 75.79 | 83.09 | 76.14

In our reproduction, with self-rewarding, the model performance on MT-Bench and IFEval continuously improves. However, there is still a gap between M3 model's performance and that of the Instruct model.

### Other Instruction Datasets

| Model | Dataset / Algorithm |  ARC-e | ARC-c | Hellaswag | SIQA | PIQA | GSM8K | MMLU | OpenBookQA | NQ
| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
| Llama-3-8b | Base(M0) | 69.84 | 45.42 | 74.68 | 46.67 | 80.96 | 55.95 | 66.62 | 50.6 | 16.09
| Llama-3-8b | IFT(SFT-baseline) | 74.43 | 47.46 | 76.99 | 49.23 | 82.81 | 57.24 | 66.36 | 52.6 | 29.58
| Llama-3-8b | EFT(M1) | 70.9 | 47.8 | 75.4 | 47.75 | 81.94 | 57.77 | 66.27 | 52 | 29.94
| Llama-3-8b | Self-Rewarding-iter1(M2) | 70.9 | 47.8 | 75.41 | 47.8 | 81.99 | 57.62 | 66.22 | 52.2 | 29.86
| Llama-3-8b | Self-Rewarding-iter1(M3) |  71.08 | 48.14 | 75.41 | 47.75 | 81.83 | 57.62 | 66.27 | 52.2 | 29.81
| Llama-3-8b | Instruct | 69.49 | 48.47 | 67.48 | 52.25 | 79 | 79.61 | 67.9 | 74 | 24.82

On 9 other instruction datasets, ift, eft and dpo maintain the performance of base model (M0).

### LLM-as-a-Judge Evaluation

| Model | Dataset / Algorithm | exact acc(%)| five fullmark(%) | spearman | kendall $\tau$
| -- | -- | -- | -- | -- | --
| Llama-3-8b | Base(M0) |  - | - | - | - | -
| Llama-3-8b | IFT(SFT-baseline) | 5.08 | 20.53 | -0.06 | -0.0507
| Llama-3-8b | EFT(M1) | 28.44 | 100 | 0.4502 | 0.3843
| Llama-3-8b | Self-Rewarding-iter1(M2) | 29.19	| 100	| 0.452 | 0.3865
| Llama-3-8b | Self-Rewarding-iter1(M3) | 27.87 | 100	 | 0.4615 | 0.3945
| Llama-3-8b | Instruct |  - | - | - | - | -

Both EFT and the subsequent self-rewarding can improve the Spearman and Kendall Tau correlation coefficients between the model scores and the human scores.
## Citation

```
@misc{yuan2024selfrewardinglanguagemodels,
      title={Self-Rewarding Language Models},
      author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason Weston},
      year={2024},
      eprint={2401.10020},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.10020},
}
```
